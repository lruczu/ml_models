{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_name = 'data/wnut17train.conll'\n",
    "valid_path_name = 'data/emerging.dev.conll'\n",
    "test_paht_name = 'data/emerging.test.annotated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(path_name: str) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    with open(path_name) as file:\n",
    "        lst_tweet_tokens = []\n",
    "        lst_tweet_tags = []\n",
    "        lst_tokens = []\n",
    "        lst_tags = []\n",
    "        for line in file:\n",
    "            token_tag_pair = line.split() \n",
    "            if len(token_tag_pair) == 2:\n",
    "                    lst_tokens.append(token_tag_pair[0])\n",
    "                    lst_tags.append(token_tag_pair[1])\n",
    "            else:\n",
    "                lst_tweet_tokens.append(lst_tokens)\n",
    "                lst_tweet_tags.append(lst_tags)\n",
    "                lst_tokens = []\n",
    "                lst_tags = []\n",
    "                \n",
    "    return lst_tweet_tokens, lst_tweet_tags\n",
    "\n",
    "def display(lst_tokens: List[str], lst_tags: List[str]) -> str:\n",
    "    result_str = ''\n",
    "    for token, tag in zip(lst_tokens, lst_tags):\n",
    "        result_str += ' ' + token + '<' + tag  + '>'\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet_tokens, train_tweet_tags = read_conll_file(train_path_name)\n",
    "valid_tweet_tokens, valid_tweet_tags = read_conll_file(valid_path_name)\n",
    "test_tweet_tokens, test_tweet_tags = read_conll_file(test_paht_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" @paulwalk<O> It<O> 's<O> the<O> view<O> from<O> where<O> I<O> 'm<O> living<O> for<O> two<O> weeks<O> .<O> Empire<B-location> State<I-location> Building<I-location> =<O> ESB<B-location> .<O> Pretty<O> bad<O> storm<O> here<O> last<O> evening<O> .<O>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(train_tweet_tokens[0], train_tweet_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training examples: 3394\n",
      "# of validating examples: 1009\n",
      "# of testing examples: 1287\n"
     ]
    }
   ],
   "source": [
    "print('# of training examples: {}'.format(len(train_tweet_tokens)))\n",
    "print('# of validating examples: {}'.format(len(valid_tweet_tokens)))\n",
    "print('# of testing examples: {}'.format(len(test_tweet_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique train tags: ['B-corporation', 'B-creative-work', 'B-group', 'B-location', 'B-person', 'B-product', 'I-corporation', 'I-creative-work', 'I-group', 'I-location', 'I-person', 'I-product', 'O']\n",
      "\n",
      "Unique valid tags: ['B-corporation', 'B-creative-work', 'B-group', 'B-location', 'B-person', 'B-product', 'I-corporation', 'I-creative-work', 'I-group', 'I-location', 'I-person', 'I-product', 'O']\n",
      "\n",
      "Unique test tags: ['B-corporation', 'B-creative-work', 'B-group', 'B-location', 'B-person', 'B-product', 'I-corporation', 'I-creative-work', 'I-group', 'I-location', 'I-person', 'I-product', 'O']\n"
     ]
    }
   ],
   "source": [
    "print('Unique train tags: {}'.format(sorted(set(reduce(lambda a, b: a + b, train_tweet_tags)))))\n",
    "print()\n",
    "print('Unique valid tags: {}'.format(sorted(set(reduce(lambda a, b: a + b, valid_tweet_tags)))))\n",
    "print()\n",
    "print('Unique test tags: {}'.format(sorted(set(reduce(lambda a, b: a + b, test_tweet_tags)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<unk>', '<pad>'] \n",
    "special_tags = ['O']\n",
    "\n",
    "def create_mappings(tweet_words: List[List[str]], \n",
    "                    special_words: List[str],\n",
    "                    normalize=True) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    returns \"word to id\" and \"id to word\"\n",
    "    \"\"\" \n",
    "    if normalize:\n",
    "        special_words = [word.lower() for word in special_words]\n",
    "        words = reduce(lambda a, b: a + b, tweet_words)\n",
    "        words = [word.lower() for word in words]\n",
    "        words = ['<user>' if word[0] == '@' else word for word in words]\n",
    "        unique_words = set(words).difference(special_words)\n",
    "    else:\n",
    "        unique_words = set(reduce(lambda a, b: a + b, tweet_words)).difference(special_words)\n",
    "        \n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    \n",
    "    for index, word in enumerate(special_words):\n",
    "        word2id[word] = index\n",
    "        id2word[index] = word\n",
    "        \n",
    "    for index_, word in enumerate(unique_words, index + 1):\n",
    "        word2id[word] = index_\n",
    "        id2word[index_] = word\n",
    "    \n",
    "    return word2id, id2word\n",
    "\n",
    "def lst2ids(lst, word2id):\n",
    "    res = []\n",
    "    for x in lst:\n",
    "        if x in word2id:\n",
    "            res.append(word2id[x])\n",
    "        else:\n",
    "            res.append(word2id['<unk>'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id, id2word = create_mappings(train_tweet_tokens, special_tokens, normalize=True)\n",
    "tag2id, id2tag = create_mappings(train_tweet_tags, special_tags, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11112, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id), len(tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crate_datasets(\n",
    "    lst_tweet_tokens,\n",
    "    lst_tweet_tags, \n",
    "    word2id,\n",
    "    id2word,\n",
    "    tag2id,\n",
    "    id2tag\n",
    "):\n",
    "    max_len = max([len(x) for x in lst_tweet_tokens])\n",
    "    size = len(lst_tweet_tokens)\n",
    "    \n",
    "    x = np.ones((size, max_len)) * word2id['<pad>']\n",
    "    y = np.ones((size, max_len)) * tag2id['O']\n",
    "    \n",
    "    lengths = np.zeros(size)\n",
    "    \n",
    "    for i, (tweet_tokens, tweet_tags) in enumerate(zip(lst_tweet_tokens, lst_tweet_tags)):\n",
    "        tweet_ids = lst2ids(tweet_tokens, word2id)\n",
    "        tweet_tags_ids = lst2ids(tweet_tags, tag2id)\n",
    "        assert len(tweet_ids) == len(tweet_tags_ids)\n",
    "        x[i, :len(tweet_ids)] = tweet_ids\n",
    "        y[i, :len(tweet_ids)] = tweet_tags_ids\n",
    "        lengths[i] = len(tweet_ids)\n",
    "        \n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, len_train = crate_datasets(train_tweet_tokens, train_tweet_tags, \n",
    "                                              word2id, id2word,\n",
    "                                              tag2id, id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, y_valid, len_valid = crate_datasets(valid_tweet_tokens, valid_tweet_tags, \n",
    "                                              word2id, id2word,\n",
    "                                              tag2id, id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3394, 41), (3394, 41), (3394,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, len_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden_units,\n",
    "        tokens_size, \n",
    "        tags_size,\n",
    "        embedding_dim,\n",
    "        rnn_cell='basic',\n",
    "        dropout_ratio=1.0,\n",
    "        learning_rate=5e-03\n",
    "    ):\n",
    "        self.rnn_cell = rnn_cell\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.tokens_size = tokens_size\n",
    "        self.tags_size = tags_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "        self.session = tf.Session(graph=self.create_graph())\n",
    "        self.session.run(self.init)\n",
    "        \n",
    "    def define_placeholders(self):\n",
    "        self.input_token_ids = tf.placeholder(shape=[None, None], \n",
    "                                             dtype=tf.int32,\n",
    "                                             name='input_token_ids')\n",
    "        \n",
    "        self.input_tag_ids = tf.placeholder(shape=[None, None],\n",
    "                                           dtype=tf.int32, \n",
    "                                           name='input_tag_ids')\n",
    "        \n",
    "        self.input_lengths = tf.placeholder(shape=[None],\n",
    "                                           dtype=tf.int32,\n",
    "                                           name='input_lengths')\n",
    "        \n",
    "        self.dropout = tf.placeholder_with_default(tf.constant(1.0,\n",
    "                                                               dtype=tf.float32\n",
    "                                                              ),\n",
    "                                                 shape=[],\n",
    "                                                 name='dropout')\n",
    "        \n",
    "        self.lr = tf.placeholder_with_default(tf.constant(1e-03, dtype=tf.float32),\n",
    "                                                        shape=[],\n",
    "                                                        name='learning_rate')\n",
    "        \n",
    "    def define_embeddings(self):\n",
    "        self.embeddings = tf.get_variable(name='embeddings',\n",
    "                                          shape=[self.tokens_size, self.embedding_dim],\n",
    "                                          dtype=tf.float32, \n",
    "                                          initializer = tf.initializers.variance_scaling()\n",
    "                                          )\n",
    "        self.embedded_input = tf.nn.embedding_lookup(\n",
    "            self.embeddings,\n",
    "            self.input_token_ids\n",
    "        )\n",
    "        \n",
    "    def define_cells(self):\n",
    "        if self.rnn_cell == 'basic':\n",
    "            self.forward_cell = tf.nn.rnn_cell.BasicRNNCell(\n",
    "                num_units=self.n_hidden_units)\n",
    "            self.backward_cell = tf.nn.rnn_cell.BasicRNNCell(\n",
    "                num_units=self.n_hidden_units)\n",
    "        \n",
    "        elif self.rnn_cell == 'lstm':\n",
    "            self.forward_cell = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "                num_units = self.n_hidden_units)\n",
    "            self.backward_cell = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "                num_units = self.n_hidden_units)\n",
    "        \n",
    "        elif self.rnn_cell == 'gru':\n",
    "            self.forward_cell = tf.nn.rnn_cell.GRUCell(\n",
    "                num_units = self.n_hidden_units)\n",
    "            self.backward_cell = tf.nn.rnn_cell.GRUCell(\n",
    "                num_units = self.n_hidden_units)\n",
    "        else:\n",
    "            raise ValueError('There is no {} for rnn_cell argument'.format(self.rnn_cell))\n",
    "        \n",
    "        self.forward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "            self.forward_cell,\n",
    "            input_keep_prob=self.dropout,\n",
    "            output_keep_prob=self.dropout,\n",
    "            state_keep_prob=self.dropout\n",
    "        )\n",
    "        \n",
    "        self.backward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "            self.backward_cell,\n",
    "            input_keep_prob=self.dropout,\n",
    "            output_keep_prob=self.dropout,\n",
    "            state_keep_prob=self.dropout\n",
    "        )\n",
    "\n",
    "        \n",
    "    def create_graph(self):\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            self.define_placeholders()\n",
    "            self.define_embeddings()\n",
    "            self.define_cells()\n",
    "\n",
    "            self.max_len = tf.shape(self.input_token_ids)[1]\n",
    "            \n",
    "            (self.fw_outputs, self.bw_outputs), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=self.forward_cell, \n",
    "                cell_bw=self.backward_cell,\n",
    "                inputs=self.embedded_input,\n",
    "                sequence_length=self.input_lengths,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "\n",
    "            self.rnn_output = tf.concat([\n",
    "                self.fw_outputs,\n",
    "                self.bw_outputs\n",
    "            ], axis=2)\n",
    "\n",
    "            self.logits = tf.layers.dense(\n",
    "                self.rnn_output,\n",
    "                self.tags_size,\n",
    "                activation=None, \n",
    "\n",
    "            )\n",
    "            \n",
    "            self.predictions = tf.argmax(self.logits, axis=2)\n",
    "\n",
    "            self.weights = tf.cast(tf.sequence_mask(self.input_lengths,\n",
    "                                                    maxlen=self.max_len),\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "            self.loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=self.logits,\n",
    "                targets=self.input_tag_ids,\n",
    "                weights=self.weights\n",
    "            )\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            clip_norm = tf.cast(1.0, tf.float32)\n",
    "\n",
    "            self.grads_and_vars = [(tf.clip_by_norm(g, clip_norm), v) for g, v in self.grads_and_vars] \n",
    "            self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
    "            \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            return graph\n",
    "                        \n",
    "    def fit(self, X, y, lengths, val_data, batch_size=64, n_epochs=10, shuffle=True):\n",
    "        with self.session.as_default() as sess:\n",
    "        \n",
    "            n = X.shape[0]\n",
    "            for i in range(n_epochs):\n",
    "                if shuffle:\n",
    "                    order = np.random.permutation(np.arange(n))\n",
    "                    X = X[order]\n",
    "                    y = y[order]\n",
    "                    lengths = lengths[order]\n",
    "                n_batches = int(n / batch_size)\n",
    " \n",
    "                for j in range(n_batches):\n",
    "                    start_index = j * batch_size\n",
    "                    end_index = (j + 1) * batch_size\n",
    "                   \n",
    "                    loss = sess.run(self.train_op, feed_dict={\n",
    "                        self.input_token_ids: X[start_index:end_index],\n",
    "                        self.input_tag_ids: y[start_index:end_index],\n",
    "                        self.input_lengths: lengths[start_index:end_index], \n",
    "                        self.dropout: self.dropout_ratio, \n",
    "                        self.lr: self.learning_rate\n",
    "                    })\n",
    "                \n",
    "                validation_loss, val_preds = sess.run([self.loss, self.predictions], feed_dict={\n",
    "                        self.input_token_ids: val_data[0],\n",
    "                        self.input_tag_ids: val_data[1],\n",
    "                        self.input_lengths: val_data[2]\n",
    "                })\n",
    "                \n",
    "                training_loss, train_preds = sess.run([self.loss, self.predictions], feed_dict={\n",
    "                        self.input_token_ids: X,\n",
    "                        self.input_tag_ids: y,\n",
    "                        self.input_lengths: lengths\n",
    "                })\n",
    "                \n",
    "                training_metric = precision_recall_fscore_support(\n",
    "                    y.flatten(),\n",
    "                    train_preds.flatten(),\n",
    "                    average='macro')\n",
    "                \n",
    "                validation_metric = precision_recall_fscore_support(\n",
    "                    val_data[1].flatten(),\n",
    "                    val_preds.flatten(),\n",
    "                    average='macro')\n",
    "                \n",
    "                print('Epoch: {}, training loss: {}'.format(i + 1, training_loss))\n",
    "                print('Epoch: {}, precision: {}'.format(i + 1, training_metric[0]))\n",
    "                print('Epoch: {}, recall: {}'.format(i + 1, training_metric[1]))\n",
    "                print('Epoch: {} f1-score: {}'.format(i + 1, training_metric[2]))\n",
    "                print('----------------')\n",
    "                print('*** validation loss: {} ***'.format(validation_loss))\n",
    "                print('*** precision: {} ***'.format(validation_metric[0]))\n",
    "                print('*** recall: {} ***'.format(validation_metric[1]))\n",
    "                print('*** f1-score: {} ***'.format(validation_metric[2]))\n",
    "                print()\n",
    "                \n",
    "    def predict(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuck",
   "language": "python",
   "name": "fuck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
